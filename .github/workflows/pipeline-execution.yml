name: Pipeline Execution (No Terraform)

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  # Resource Groups
  MAIN_RESOURCE_GROUP: "de-log-processing-rg"
  DATABRICKS_RESOURCE_GROUP: "DE-Log-Processing-rg-dbf6d727"
  
  # Resource Names
  STORAGE_ACCOUNT_NAME: "delogprocessingdatalake"
  DATABRICKS_WORKSPACE: "adb-4365470537346742.2"
  AKS_CLUSTER_NAME: "de-log-processing-aks"
  CONTAINER_REGISTRY: "delogprocessingacrjs9d"
  KEY_VAULT_NAME: "de-log-processing-kv"

jobs:
  lint-and-validate:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install flake8
    
    - name: Lint code
      run: |
        flake8 scripts/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 scripts/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Validate code structure
      run: |
        echo "Validating code structure..."
        python -c "import scripts.bronze.raw_load; print('Bronze script OK')"
        python -c "import scripts.silver.silver_load; print('Silver script OK')"
        python -c "import scripts.gold.gold_load; print('Gold script OK')"
        python -c "import scripts.ml_anomaly_detection; print('ML script OK')"

  upload-source-data:
    needs: lint-and-validate
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Check if source data exists and upload if needed
      run: |
        echo "Checking if source data already exists in Azure Storage..."
        
        # Create containers if they don't exist
        az storage container create --name bronze --account-name ${{ env.STORAGE_ACCOUNT_NAME }} --auth-mode key --account-key ${{ secrets.STORAGE_ACCOUNT_KEY }}
        az storage container create --name silver --account-name ${{ env.STORAGE_ACCOUNT_NAME }} --auth-mode key --account-key ${{ secrets.STORAGE_ACCOUNT_KEY }}
        az storage container create --name gold --account-name ${{ env.STORAGE_ACCOUNT_NAME }} --auth-mode key --account-key ${{ secrets.STORAGE_ACCOUNT_KEY }}
        
        # Check if source data already exists
        if az storage blob exists --account-name ${{ env.STORAGE_ACCOUNT_NAME }} --container-name bronze --name raw_logs/OpenSSH_2k.log --auth-mode key --account-key ${{ secrets.STORAGE_ACCOUNT_KEY }} | grep -q "true"; then
          echo "âœ… Source data already exists in Azure Storage - skipping upload"
        else
          echo "ðŸ“¤ Uploading source data to Azure Storage..."
          az storage blob upload --file logs/OpenSSH_2k.log --container-name bronze --name raw_logs/OpenSSH_2k.log --account-name ${{ env.STORAGE_ACCOUNT_NAME }} --auth-mode key --account-key ${{ secrets.STORAGE_ACCOUNT_KEY }}
          echo "âœ… Source data uploaded successfully!"
        fi

  deploy-databricks-pipeline:
    needs: upload-source-data
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Install Databricks CLI
      run: |
        echo "Installing Databricks CLI..."
        pip install databricks-cli
        databricks --version
    
    - name: Deploy PySpark scripts to Databricks
      run: |
        echo "Deploying PySpark scripts to Databricks..."
        
        # Set Databricks configuration
        export DATABRICKS_HOST="https://${{ env.DATABRICKS_WORKSPACE }}.azuredatabricks.net"
        export DATABRICKS_TOKEN="${{ secrets.DATABRICKS_TOKEN }}"
        
        # Create workspace directories
        databricks workspace mkdirs /DE-Log-Processing
        databricks workspace mkdirs /DE-Log-Processing/bronze
        databricks workspace mkdirs /DE-Log-Processing/silver
        databricks workspace mkdirs /DE-Log-Processing/gold
        databricks workspace mkdirs /DE-Log-Processing/ml
        
        # Upload PySpark scripts
        databricks workspace import scripts/bronze/raw_load.py /DE-Log-Processing/bronze/raw_load.py --language PYTHON
        databricks workspace import scripts/silver/silver_load.py /DE-Log-Processing/silver/silver_load.py --language PYTHON
        databricks workspace import scripts/gold/gold_load.py /DE-Log-Processing/gold/gold_load.py --language PYTHON
        databricks workspace import scripts/ml_anomaly_detection.py /DE-Log-Processing/ml/ml_anomaly_detection.py --language PYTHON
        
        echo "PySpark scripts deployed to Databricks successfully!"

  deploy-aks-services:
    needs: deploy-databricks-pipeline
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Get AKS credentials
      run: |
        az aks get-credentials --resource-group ${{ env.MAIN_RESOURCE_GROUP }} --name ${{ env.AKS_CLUSTER_NAME }}
    
    - name: Deploy MVP Pods to AKS
      run: |
        echo "Deploying MVP pods to AKS..."
        
        # Deploy Prometheus for monitoring
        kubectl apply -f kubernetes/prometheus-pod.yaml
        echo "âœ… Prometheus deployed"
        
        # Deploy Grafana for visualization
        kubectl apply -f kubernetes/grafana-pod.yaml
        echo "âœ… Grafana deployed"
        
        # Deploy Pytest pod for testing
        kubectl apply -f kubernetes/pytest-pod.yaml
        echo "âœ… Pytest pod deployed"
        
        # Wait for deployments to be ready
        kubectl wait --for=condition=available --timeout=300s deployment/prometheus
        kubectl wait --for=condition=available --timeout=300s deployment/grafana
        kubectl wait --for=condition=ready --timeout=300s pod/pytest-pod
        
        echo "ðŸŽ¯ MVP pods deployed to AKS successfully!"
        echo "ðŸ“Š Prometheus: Monitoring pipeline metrics"
        echo "ðŸ“ˆ Grafana: Visualizing key metrics dashboard"
        echo "ðŸ§ª Pytest: Code coverage and testing"

  execute-pipeline:
    needs: deploy-aks-services
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Execute Bronze Layer
      run: |
        echo "Executing Bronze Layer on Databricks..."
        
        # Set Databricks configuration
        export DATABRICKS_HOST="https://${{ env.DATABRICKS_WORKSPACE }}.azuredatabricks.net"
        export DATABRICKS_TOKEN="${{ secrets.DATABRICKS_TOKEN }}"
        
        # Submit Bronze layer job
        databricks jobs submit --json '{
          "name": "Bronze Layer - Raw Data Processing",
          "new_cluster": {
            "spark_version": "13.3.x-scala2.12",
            "node_type_id": "Standard_DS3_v2",
            "num_workers": 2
          },
          "notebook_task": {
            "notebook_path": "/DE-Log-Processing/bronze/raw_load"
          }
        }'
        
        echo "Bronze Layer execution completed!"
    
    - name: Execute Silver Layer
      run: |
        echo "Executing Silver Layer on Databricks..."
        
        # Submit Silver layer job
        databricks jobs submit --json '{
          "name": "Silver Layer - Data Transformation",
          "new_cluster": {
            "spark_version": "13.3.x-scala2.12",
            "node_type_id": "Standard_DS3_v2",
            "num_workers": 2
          },
          "notebook_task": {
            "notebook_path": "/DE-Log-Processing/silver/silver_load"
          }
        }'
        
        echo "Silver Layer execution completed!"
    
    - name: Execute Gold Layer
      run: |
        echo "Executing Gold Layer on Databricks..."
        
        # Submit Gold layer job
        databricks jobs submit --json '{
          "name": "Gold Layer - Final Data Processing",
          "new_cluster": {
            "spark_version": "13.3.x-scala2.12",
            "node_type_id": "Standard_DS3_v2",
            "num_workers": 2
          },
          "notebook_task": {
            "notebook_path": "/DE-Log-Processing/gold/gold_load"
          }
        }'
        
        echo "Gold Layer execution completed!"
    
    - name: Execute ML Pipeline with MLflow
      run: |
        echo "Executing ML Pipeline on Databricks with MLflow..."
        
        # Submit ML job with MLflow integration
        databricks jobs submit --json '{
          "name": "ML Pipeline - Anomaly Detection with MLflow",
          "new_cluster": {
            "spark_version": "13.3.x-scala2.12",
            "node_type_id": "Standard_DS3_v2",
            "num_workers": 2,
            "spark_conf": {
              "spark.databricks.mlflow.enabled": "true"
            }
          },
          "notebook_task": {
            "notebook_path": "/DE-Log-Processing/ml/ml_anomaly_detection"
          },
          "libraries": [
            {
              "pypi": {
                "package": "mlflow"
              }
            }
          ]
        }'
        
        echo "ML Pipeline with MLflow execution completed!"
        echo "ðŸŽ¯ Model trained and registered in Databricks MLflow"
        echo "ðŸ“Š Model serving available via MLflow endpoints"

  validate-pipeline-execution:
    needs: execute-pipeline
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Validate MVP Deployment
      run: |
        echo "Validating MVP deployment..."
        
        # Check if data exists in storage
        az storage blob exists --account-name ${{ env.STORAGE_ACCOUNT_NAME }} --container-name bronze --name raw_logs/OpenSSH_2k.log --auth-mode key --account-key ${{ secrets.STORAGE_ACCOUNT_KEY }}
        az storage blob exists --account-name ${{ env.STORAGE_ACCOUNT_NAME }} --container-name silver --name structured_logs.json --auth-mode key --account-key ${{ secrets.STORAGE_ACCOUNT_KEY }}
        az storage blob exists --account-name ${{ env.STORAGE_ACCOUNT_NAME }} --container-name gold --name openssh_logs_final.csv --auth-mode key --account-key ${{ secrets.STORAGE_ACCOUNT_KEY }}
        
        # Check AKS MVP pods
        az aks get-credentials --resource-group ${{ env.MAIN_RESOURCE_GROUP }} --name ${{ env.AKS_CLUSTER_NAME }}
        
        echo "Checking MVP pods status..."
        kubectl get pods -l app=prometheus
        kubectl get pods -l app=grafana
        kubectl get pods -l app=pytest
        
        echo "Checking services..."
        kubectl get services
        
        echo "Checking Prometheus metrics endpoint..."
        kubectl port-forward service/prometheus-service 9090:9090 &
        sleep 10
        curl -f http://localhost:9090/metrics || echo "Prometheus metrics not ready yet"
        
        echo "MVP deployment validation completed successfully!"
        echo "ðŸŽ¯ Databricks: ETL + ML + MLflow"
        echo "ðŸ“Š Prometheus: Pipeline monitoring"
        echo "ðŸ“ˆ Grafana: Metrics visualization"
        echo "ðŸ§ª Pytest: Code testing"

  test-pipeline:
    needs: validate-pipeline-execution
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Test Pipeline Execution
      run: |
        echo "Testing pipeline execution..."
        echo "Checking source data in Azure Storage..."
        az storage blob exists --account-name ${{ env.STORAGE_ACCOUNT_NAME }} --container-name bronze --name raw_logs/OpenSSH_2k.log --auth-mode key --account-key ${{ secrets.STORAGE_ACCOUNT_KEY }}
        echo "Checking Databricks workspace..."
        export DATABRICKS_HOST="https://${{ env.DATABRICKS_WORKSPACE }}.azuredatabricks.net"
        export DATABRICKS_TOKEN="${{ secrets.DATABRICKS_TOKEN }}"
        databricks workspace list --path /
        echo "Checking AKS services..."
        az aks get-credentials --resource-group ${{ env.MAIN_RESOURCE_GROUP }} --name ${{ env.AKS_CLUSTER_NAME }}
        kubectl get pods
        kubectl get services
    
    - name: Run Comprehensive Tests
      run: |
        echo "Running comprehensive pipeline tests..."
        python -m pytest tests/test_pipeline.py --cov=scripts --cov-report=xml --cov-report=term-missing -v
    
    - name: Upload Test Results
      uses: actions/upload-artifact@v4
      with:
        name: test-results
        path: |
          coverage.xml
          .coverage
          htmlcov/
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: integration-tests
        name: codecov-pipeline-tests

  cleanup:
    if: always()
    needs: [lint-and-validate, upload-source-data, deploy-databricks-pipeline, deploy-aks-services, execute-pipeline, validate-pipeline-execution, test-pipeline]
    runs-on: ubuntu-latest
    steps:
    - name: Cleanup temporary files
      run: |
        echo "Cleaning up temporary files..."
        # Add any cleanup steps here if needed
