# Testing Documentation - DE Log Processing & ML Pipeline

**Author:** Aditya Padhi

This document provides a comprehensive overview of the testing strategy, implementation, and results for the DE Log Processing & ML Pipeline.

---

## üìã Table of Contents

1. [Testing Overview](#testing-overview)
2. [Test Categories](#test-categories)
3. [Test Results](#test-results)
4. [Code Coverage Analysis](#code-coverage-analysis)
5. [Running Tests](#running-tests)
6. [Test Files Structure](#test-files-structure)
7. [What's Tested vs What's Not](#whats-tested-vs-whats-not)
8. [Future Improvements](#future-improvements)

---

## Testing Overview

### Testing Philosophy
Our testing approach focuses on **integration testing** and **data validation** rather than unit testing individual functions. This approach is ideal for data pipelines because:

- ‚úÖ **Validates actual pipeline outputs**
- ‚úÖ **Ensures data quality and integrity**
- ‚úÖ **Verifies ML model functionality**
- ‚úÖ **Production-ready validation**

### Testing Tools
- **Framework:** pytest 7.4.4
- **Coverage:** pytest-cov 7.0.0
- **Configuration:** pytest.ini
- **Test Location:** `tests/test_pipeline.py`

---

## Test Categories

### 1. üîç Data Validation Tests (3 tests)

**Purpose:** Validate data format and structure

| Test | Description | Validates |
|------|-------------|-----------|
| `test_time_format_validation` | Time format validation | HH:MM:SS format using regex |
| `test_pid_validation` | Process ID validation | Numeric, non-empty PIDs |
| `test_month_validation` | Month abbreviation validation | Valid month names (Jan-Dec) |

**Example:**
```python
def test_time_format_validation(self):
    valid_times = ["06:55:46", "23:59:59", "00:00:00", "12:30:45"]
    for time_str in valid_times:
        pattern = r'^\d{2}:\d{2}:\d{2}$'
        assert re.match(pattern, time_str)
```

### 2. üìÅ File Operations Tests (3 tests)

**Purpose:** Verify file existence and structure

| Test | Description | Validates |
|------|-------------|-----------|
| `test_source_log_file_exists` | Source file validation | `logs/OpenSSH_2k.log` exists and not empty |
| `test_data_directories_exist` | Directory structure | All required directories exist |
| `test_output_files_exist` | Output files validation | All pipeline outputs exist and not empty |

**Directories Checked:**
- `data/bronze/`
- `data/silver/`
- `data/gold/`
- `data/ml_output/`

**Files Checked:**
- `data/bronze/raw_logs.parquet`
- `data/silver/structured_logs.json`
- `data/gold/openssh_logs_final.csv`
- `data/ml_output/anomaly_predictions.csv`
- `data/ml_output/detected_anomalies.csv`
- `data/ml_output/ml_summary_report.txt`

### 3. ü§ñ ML Model Tests (3 tests)

**Purpose:** Validate ML model and predictions

| Test | Description | Validates |
|------|-------------|-----------|
| `test_model_file_exists` | Model file validation | `models/anomaly_model.pkl` exists and not empty |
| `test_predictions_format` | Predictions structure | Required columns, numeric scores, binary flags |
| `test_anomaly_detection_results` | Anomaly results | Negative scores, anomaly flags, non-empty results |

**Predictions Validation:**
```python
# Required columns
required_columns = ["LineId", "datetime", "EventId", "is_anomaly", "anomaly_score"]

# Data type validation
assert pd.api.types.is_numeric_dtype(df['anomaly_score'])
assert df['is_anomaly'].isin([0, 1]).all()
```

### 4. üìä Data Quality Tests (2 tests)

**Purpose:** Validate data quality and format

| Test | Description | Validates |
|------|-------------|-----------|
| `test_gold_layer_data_quality` | Gold layer validation | Required columns, no nulls, datetime format |
| `test_silver_layer_json_format` | Silver layer validation | JSONL format, required fields |

**Gold Layer Validation:**
- Required columns: `LineId`, `datetime`, `Component`, `Pid`, `EventId`, `EventTemplate`, `Content`
- No null values in critical columns
- Proper datetime format (`dd-mm-yyyy : hh:mm:ss`)

**Silver Layer Validation:**
- JSONL format (one JSON object per line)
- Required fields in each record
- Valid JSON structure

---

## Test Results

### Current Status
```
============================== test session starts ==============================
platform darwin -- Python 3.12.2, pytest-7.4.4
collected 11 items

tests/test_pipeline.py::TestDataValidation::test_time_format_validation PASSED [  9%]
tests/test_pipeline.py::TestDataValidation::test_pid_validation PASSED   [ 18%]
tests/test_pipeline.py::TestDataValidation::test_month_validation PASSED [ 27%]
tests/test_pipeline.py::TestFileOperations::test_source_log_file_exists PASSED [ 36%]
tests/test_pipeline.py::TestFileOperations::test_data_directories_exist PASSED [ 45%]
tests/test_pipeline.py::TestFileOperations::test_output_files_exist PASSED [ 54%]
tests/test_pipeline.py::TestMLModel::test_model_file_exists PASSED       [ 63%]
tests/test_pipeline.py::TestMLModel::test_predictions_format PASSED      [ 72%]
tests/test_pipeline.py::TestMLModel::test_anomaly_detection_results PASSED [ 81%]
tests/test_pipeline.py::TestDataQuality::test_gold_layer_data_quality PASSED [ 90%]
tests/test_pipeline.py::TestDataQuality::test_silver_layer_json_format PASSED [100%]

============================== 11 passed in 0.52s ==============================
```

### Summary Statistics
- **Total Tests:** 11
- **Passed:** 11 ‚úÖ
- **Failed:** 0 ‚ùå
- **Success Rate:** 100%
- **Execution Time:** 0.52 seconds

---

## Code Coverage Analysis

### Coverage Report
```
Name                              Stmts   Miss  Cover   Missing
---------------------------------------------------------------
scripts/bronze/__init__.py            0      0   100%
scripts/bronze/raw_load.py           50     50     0%   13-133
scripts/generate_sample_data.py     138    138     0%   2-245
scripts/gold/__init__.py              0      0   100%
scripts/gold/gold_load.py           101    101     0%   19-311
scripts/ml_anomaly_detection.py     153    153     0%   21-323
scripts/raw_to_bronze.py             79     79     0%   12-215
scripts/raw_to_bronze_pandas.py     124    124     0%   15-264
scripts/run_pipeline.py              85     85     0%   20-166
scripts/run_pipeline_pandas.py      140    140     0%   13-246
scripts/silver/__init__.py            0      0   100%
scripts/silver/silver_load.py        99     99     0%   14-405
---------------------------------------------------------------
TOTAL                               969    969     0%
```

### Coverage Interpretation
- **Total Lines:** 969
- **Covered Lines:** 0 (0%)
- **Uncovered Lines:** 969 (100%)

**Why 0% Coverage?**
- Tests validate **outputs** and **data quality**
- Tests don't execute the actual pipeline scripts
- This is **integration testing** approach
- Focus on **data validation** rather than code execution

---

## Running Tests

### Prerequisites
```bash
# Install pytest and coverage tools
pip install pytest pytest-cov

# Ensure pipeline has been run
python scripts/run_pipeline_pandas.py
python scripts/ml_anomaly_detection.py
```

### Basic Test Execution
```bash
# Run all tests
python -m pytest tests/test_pipeline.py -v

# Run specific test class
python -m pytest tests/test_pipeline.py::TestDataValidation -v

# Run specific test
python -m pytest tests/test_pipeline.py::TestDataValidation::test_time_format_validation -v
```

### Test Execution with Coverage
```bash
# Run tests with coverage report
python -m pytest tests/test_pipeline.py --cov=scripts --cov-report=term-missing -v

# Generate HTML coverage report
python -m pytest tests/test_pipeline.py --cov=scripts --cov-report=html:htmlcov -v
```

### Test Configuration
Tests use `pytest.ini` configuration:
```ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --strict-markers
    --disable-warnings
    --cov=scripts
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-fail-under=70
```

---

## Test Files Structure

```
tests/
‚îú‚îÄ‚îÄ __init__.py                 # Test package initialization
‚îú‚îÄ‚îÄ test_pipeline.py           # Main test file (11 tests)
‚îú‚îÄ‚îÄ fixtures/                  # Test data fixtures (empty)
‚îú‚îÄ‚îÄ integration/               # Integration tests (empty)
‚îî‚îÄ‚îÄ unit/                      # Unit tests (empty)

pytest.ini                     # Pytest configuration
```

### Test File Organization
- **`test_pipeline.py`:** Main test file with all 11 tests
- **`fixtures/`:** Directory for test data (currently empty)
- **`integration/`:** Directory for integration tests (currently empty)
- **`unit/`:** Directory for unit tests (currently empty)

---

## What's Tested vs What's Not

### ‚úÖ What's Tested (Integration Testing)

**Data Validation:**
- Time format validation (HH:MM:SS)
- PID validation (numeric, non-empty)
- Month validation (valid abbreviations)

**File Operations:**
- Source file existence and content
- Directory structure validation
- Output file existence and content

**ML Model:**
- Model file existence
- Predictions format and data types
- Anomaly detection results validation

**Data Quality:**
- Gold layer CSV structure and content
- Silver layer JSON format and fields
- Data integrity checks

### ‚ùå What's Not Tested (Unit Testing)

**Individual Functions:**
- PySpark transformation functions
- Data parsing logic
- Quality check implementations
- ML model training functions

**Error Handling:**
- Exception handling paths
- Error recovery mechanisms
- Input validation edge cases

**Performance:**
- Execution time validation
- Memory usage checks
- Scalability testing

**Integration Points:**
- PySpark job execution
- File I/O operations
- Database connections

---

## Future Improvements

### Phase 1: Unit Testing (Recommended)
```python
# Example unit test structure
def test_parse_log_line():
    """Test individual log parsing function"""
    log_line = "Dec 10 06:55:46 LabSZ sshd[24200]: reverse mapping..."
    result = parse_log_line(log_line)
    assert result['Date'] == 'Dec'
    assert result['Day'] == '10'
    assert result['Time'] == '06:55:46'

def test_quality_check_completeness():
    """Test completeness quality check"""
    data = {'Date': 'Dec', 'Time': '', 'Pid': '123'}
    result = check_completeness(data)
    assert result == False  # Empty Time should fail
```

### Phase 2: Mock Testing
```python
# Example mock test for PySpark
@patch('pyspark.sql.SparkSession')
def test_bronze_layer_execution(mock_spark):
    """Test Bronze layer with mocked PySpark"""
    # Mock SparkSession and DataFrame operations
    # Test without actual Spark execution
```

### Phase 3: Performance Testing
```python
def test_pipeline_performance():
    """Test pipeline execution time"""
    start_time = time.time()
    run_pipeline()
    execution_time = time.time() - start_time
    assert execution_time < 10  # Should complete in under 10 seconds
```

### Phase 4: Error Handling Tests
```python
def test_invalid_log_format():
    """Test handling of invalid log formats"""
    with pytest.raises(ValueError):
        parse_invalid_log("invalid log line")
```

---

## üéØ Testing Strategy Summary

### Current Approach: Integration Testing ‚úÖ
- **Focus:** Data validation and output verification
- **Coverage:** Pipeline results and data quality
- **Benefits:** Production-ready validation, data integrity assurance
- **Use Case:** Perfect for data pipeline validation

### Future Approach: Comprehensive Testing üîÑ
- **Unit Tests:** Individual function validation
- **Mock Tests:** PySpark operation testing
- **Performance Tests:** Execution time validation
- **Error Tests:** Exception handling verification

### Recommended Next Steps
1. **Add unit tests** for critical functions
2. **Implement mock tests** for PySpark operations
3. **Add performance benchmarks**
4. **Create error handling tests**

---

## üìö Additional Resources

### Test Commands Reference
```bash
# Quick test run
pytest tests/test_pipeline.py -v

# Coverage analysis
pytest tests/test_pipeline.py --cov=scripts --cov-report=term-missing

# HTML coverage report
pytest tests/test_pipeline.py --cov=scripts --cov-report=html:htmlcov

# Run specific test category
pytest tests/test_pipeline.py::TestDataValidation -v
pytest tests/test_pipeline.py::TestMLModel -v
```

### Test Data Requirements
- Source log file: `logs/OpenSSH_2k.log`
- Pipeline outputs in `data/` directories
- ML model in `models/anomaly_model.pkl`

### Continuous Integration
Tests are ready for CI/CD integration:
- GitHub Actions compatible
- Clear pass/fail criteria
- Comprehensive validation
- Professional testing standards

---

**Last Updated:** October 16, 2025  
**Test Framework:** pytest 7.4.4  
**Coverage Tool:** pytest-cov 7.0.0  
**Total Tests:** 11 (100% pass rate)
